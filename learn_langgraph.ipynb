{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the simplest Graph\n",
    "\n",
    "We start with a graph with two nodes connected by one edge. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langgraph in ./.langgraph-notebook/lib/python3.11/site-packages (0.0.38)\n",
      "Requirement already satisfied: langchain-core<0.2.0,>=0.1.42 in ./.langgraph-notebook/lib/python3.11/site-packages (from langgraph) (0.1.45)\n",
      "Requirement already satisfied: PyYAML>=5.3 in ./.langgraph-notebook/lib/python3.11/site-packages (from langchain-core<0.2.0,>=0.1.42->langgraph) (6.0.1)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in ./.langgraph-notebook/lib/python3.11/site-packages (from langchain-core<0.2.0,>=0.1.42->langgraph) (1.33)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.0 in ./.langgraph-notebook/lib/python3.11/site-packages (from langchain-core<0.2.0,>=0.1.42->langgraph) (0.1.49)\n",
      "Requirement already satisfied: packaging<24.0,>=23.2 in ./.langgraph-notebook/lib/python3.11/site-packages (from langchain-core<0.2.0,>=0.1.42->langgraph) (23.2)\n",
      "Requirement already satisfied: pydantic<3,>=1 in ./.langgraph-notebook/lib/python3.11/site-packages (from langchain-core<0.2.0,>=0.1.42->langgraph) (2.7.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in ./.langgraph-notebook/lib/python3.11/site-packages (from langchain-core<0.2.0,>=0.1.42->langgraph) (8.2.3)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in ./.langgraph-notebook/lib/python3.11/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.2.0,>=0.1.42->langgraph) (2.4)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in ./.langgraph-notebook/lib/python3.11/site-packages (from langsmith<0.2.0,>=0.1.0->langchain-core<0.2.0,>=0.1.42->langgraph) (3.10.1)\n",
      "Requirement already satisfied: requests<3,>=2 in ./.langgraph-notebook/lib/python3.11/site-packages (from langsmith<0.2.0,>=0.1.0->langchain-core<0.2.0,>=0.1.42->langgraph) (2.31.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in ./.langgraph-notebook/lib/python3.11/site-packages (from pydantic<3,>=1->langchain-core<0.2.0,>=0.1.42->langgraph) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.1 in ./.langgraph-notebook/lib/python3.11/site-packages (from pydantic<3,>=1->langchain-core<0.2.0,>=0.1.42->langgraph) (2.18.1)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in ./.langgraph-notebook/lib/python3.11/site-packages (from pydantic<3,>=1->langchain-core<0.2.0,>=0.1.42->langgraph) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.langgraph-notebook/lib/python3.11/site-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.0->langchain-core<0.2.0,>=0.1.42->langgraph) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.langgraph-notebook/lib/python3.11/site-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.0->langchain-core<0.2.0,>=0.1.42->langgraph) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.langgraph-notebook/lib/python3.11/site-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.0->langchain-core<0.2.0,>=0.1.42->langgraph) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.langgraph-notebook/lib/python3.11/site-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.0->langchain-core<0.2.0,>=0.1.42->langgraph) (2024.2.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install langgraph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nodes act like functions that can be called as needed. In our case Node 1 is our starting point and Node 2 is our finish point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_1(input_1):\n",
    "    return input_1 + \" Hi \"\n",
    "\n",
    "def function_2(input_2):\n",
    "    return input_2 + \"there\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import Graph\n",
    "\n",
    "# Define a Langchain graph\n",
    "workflow = Graph()\n",
    "\n",
    "workflow.add_node(\"node_1\", function_1)\n",
    "workflow.add_node(\"node_2\", function_2)\n",
    "\n",
    "workflow.add_edge('node_1', 'node_2')\n",
    "\n",
    "workflow.set_entry_point(\"node_1\")\n",
    "workflow.set_finish_point(\"node_2\")\n",
    "\n",
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello Hi there'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "app.invoke(\"Hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output from node 'node_1':\n",
      "---\n",
      "Hello Hi \n",
      "\n",
      "---\n",
      "\n",
      "Output from node 'node_2':\n",
      "---\n",
      "Hello Hi there\n",
      "\n",
      "---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input = 'Hello'\n",
    "for output in app.stream(input):\n",
    "    # stream() yields dictionaries with output keyed by node name\n",
    "    for key, value in output.items():\n",
    "        print(f\"Output from node '{key}':\")\n",
    "        print(\"---\")\n",
    "        print(value)\n",
    "    print(\"\\n---\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### As you can see, we can run the nodes as functions and return some values from them. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding LLM Call\n",
    "\n",
    "Now, let's make the first node as an \"Agent\" that can call Open AI models. We can use langchain to make this call easy for us. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in ./.langgraph-notebook/lib/python3.11/site-packages (0.1.16)\n",
      "Requirement already satisfied: langchain_openai in ./.langgraph-notebook/lib/python3.11/site-packages (0.1.3)\n",
      "Requirement already satisfied: PyYAML>=5.3 in ./.langgraph-notebook/lib/python3.11/site-packages (from langchain) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in ./.langgraph-notebook/lib/python3.11/site-packages (from langchain) (2.0.29)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in ./.langgraph-notebook/lib/python3.11/site-packages (from langchain) (3.9.5)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in ./.langgraph-notebook/lib/python3.11/site-packages (from langchain) (0.6.4)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in ./.langgraph-notebook/lib/python3.11/site-packages (from langchain) (1.33)\n",
      "Requirement already satisfied: langchain-community<0.1,>=0.0.32 in ./.langgraph-notebook/lib/python3.11/site-packages (from langchain) (0.0.34)\n",
      "Requirement already satisfied: langchain-core<0.2.0,>=0.1.42 in ./.langgraph-notebook/lib/python3.11/site-packages (from langchain) (0.1.45)\n",
      "Requirement already satisfied: langchain-text-splitters<0.1,>=0.0.1 in ./.langgraph-notebook/lib/python3.11/site-packages (from langchain) (0.0.1)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in ./.langgraph-notebook/lib/python3.11/site-packages (from langchain) (0.1.49)\n",
      "Requirement already satisfied: numpy<2,>=1 in ./.langgraph-notebook/lib/python3.11/site-packages (from langchain) (1.26.4)\n",
      "Requirement already satisfied: pydantic<3,>=1 in ./.langgraph-notebook/lib/python3.11/site-packages (from langchain) (2.7.0)\n",
      "Requirement already satisfied: requests<3,>=2 in ./.langgraph-notebook/lib/python3.11/site-packages (from langchain) (2.31.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in ./.langgraph-notebook/lib/python3.11/site-packages (from langchain) (8.2.3)\n",
      "Requirement already satisfied: openai<2.0.0,>=1.10.0 in ./.langgraph-notebook/lib/python3.11/site-packages (from langchain_openai) (1.23.2)\n",
      "Requirement already satisfied: tiktoken<1,>=0.5.2 in ./.langgraph-notebook/lib/python3.11/site-packages (from langchain_openai) (0.6.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./.langgraph-notebook/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./.langgraph-notebook/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./.langgraph-notebook/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./.langgraph-notebook/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in ./.langgraph-notebook/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in ./.langgraph-notebook/lib/python3.11/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (3.21.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in ./.langgraph-notebook/lib/python3.11/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (0.9.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in ./.langgraph-notebook/lib/python3.11/site-packages (from jsonpatch<2.0,>=1.33->langchain) (2.4)\n",
      "Requirement already satisfied: packaging<24.0,>=23.2 in ./.langgraph-notebook/lib/python3.11/site-packages (from langchain-core<0.2.0,>=0.1.42->langchain) (23.2)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in ./.langgraph-notebook/lib/python3.11/site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.1)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in ./.langgraph-notebook/lib/python3.11/site-packages (from openai<2.0.0,>=1.10.0->langchain_openai) (4.3.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in ./.langgraph-notebook/lib/python3.11/site-packages (from openai<2.0.0,>=1.10.0->langchain_openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in ./.langgraph-notebook/lib/python3.11/site-packages (from openai<2.0.0,>=1.10.0->langchain_openai) (0.27.0)\n",
      "Requirement already satisfied: sniffio in ./.langgraph-notebook/lib/python3.11/site-packages (from openai<2.0.0,>=1.10.0->langchain_openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in ./.langgraph-notebook/lib/python3.11/site-packages (from openai<2.0.0,>=1.10.0->langchain_openai) (4.66.2)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in ./.langgraph-notebook/lib/python3.11/site-packages (from openai<2.0.0,>=1.10.0->langchain_openai) (4.11.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in ./.langgraph-notebook/lib/python3.11/site-packages (from pydantic<3,>=1->langchain) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.1 in ./.langgraph-notebook/lib/python3.11/site-packages (from pydantic<3,>=1->langchain) (2.18.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.langgraph-notebook/lib/python3.11/site-packages (from requests<3,>=2->langchain) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.langgraph-notebook/lib/python3.11/site-packages (from requests<3,>=2->langchain) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.langgraph-notebook/lib/python3.11/site-packages (from requests<3,>=2->langchain) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.langgraph-notebook/lib/python3.11/site-packages (from requests<3,>=2->langchain) (2024.2.2)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in ./.langgraph-notebook/lib/python3.11/site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
      "Requirement already satisfied: regex>=2022.1.18 in ./.langgraph-notebook/lib/python3.11/site-packages (from tiktoken<1,>=0.5.2->langchain_openai) (2024.4.16)\n",
      "Requirement already satisfied: httpcore==1.* in ./.langgraph-notebook/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.10.0->langchain_openai) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in ./.langgraph-notebook/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.10.0->langchain_openai) (0.14.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in ./.langgraph-notebook/lib/python3.11/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (1.0.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain langchain_openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A usual call to ChatOpenAI model in LangChain is done as below:\n",
    "\n",
    "First set your API keys for OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-dotenv in ./.langgraph-notebook/lib/python3.11/site-packages (1.0.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sk-proj-mQR5idb5CUDbDvvcmF6kT3BlbkFJJoisl83UixsGGhf008lU\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "print(os.environ.get(\"OPENAI_API_KEY\"))\n",
    "\n",
    "# Now you can access your environment variables using os.environ\n",
    "os.environ['OPENAI_API_KEY'] = os.environ.get(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello! How can I assist you today?', response_metadata={'token_usage': {'completion_tokens': 9, 'prompt_tokens': 9, 'total_tokens': 18}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_c2295e73ad', 'finish_reason': 'stop', 'logprobs': None}, id='run-2641b872-f614-4ebf-b797-6b6ef7ec34d3-0')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Set the model as ChatOpenAI\n",
    "model = ChatOpenAI(temperature=0) \n",
    "\n",
    "#Call the model with a user message\n",
    "model.invoke('Hey there')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And if you just want to see the AI response, you can do the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello! How can I assist you today?'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.invoke('Hey there').content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool! Keeping that in mind, let's change the function 1 above so that we can send the user question to the model. Then we will send this response to function 2, which will add a short string and return to the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_1(input_1):\n",
    "    response = model.invoke(input_1)\n",
    "    return response.content\n",
    "\n",
    "def function_2(input_2):\n",
    "    return \"Agent Says: \" + input_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a Langchain graph\n",
    "workflow = Graph()\n",
    "\n",
    "#calling node 1 as agent\n",
    "workflow.add_node(\"agent\", function_1)\n",
    "workflow.add_node(\"node_2\", function_2)\n",
    "\n",
    "workflow.add_edge('agent', 'node_2')\n",
    "\n",
    "workflow.set_entry_point(\"agent\")\n",
    "workflow.set_finish_point(\"node_2\")\n",
    "\n",
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Agent Says: Hello! How can I assist you today?'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "app.invoke(\"Hey there\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output from node 'agent':\n",
      "---\n",
      "Hello! How can I assist you today?\n",
      "\n",
      "---\n",
      "\n",
      "Output from node 'node_2':\n",
      "---\n",
      "Agent Says: Hello! How can I assist you today?\n",
      "\n",
      "---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input = 'Hey there'\n",
    "for output in app.stream(input):\n",
    "    # stream() yields dictionaries with output keyed by node name\n",
    "    for key, value in output.items():\n",
    "        print(f\"Output from node '{key}':\")\n",
    "        print(\"---\")\n",
    "        print(value)\n",
    "    print(\"\\n---\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First functional Agent App - City Temperature\n",
    "\n",
    "### Step 1: Parse the city mentioned\n",
    "Let's extract the city that a user mentions in a query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_1(input_1):\n",
    "    complete_query = \"Your task is to provide only the city name based on the user query. \\\n",
    "        Nothing more, just the city name mentioned. Following is the user query: \" + input_1\n",
    "    response = model.invoke(complete_query)\n",
    "    return response.content\n",
    "\n",
    "def function_2(input_2):\n",
    "    return \"Agent Says: \" + input_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a Langchain graph\n",
    "workflow = Graph()\n",
    "\n",
    "#calling node 1 as agent\n",
    "workflow.add_node(\"agent\", function_1)\n",
    "workflow.add_node(\"node_2\", function_2)\n",
    "\n",
    "workflow.add_edge('agent', 'node_2')\n",
    "\n",
    "workflow.set_entry_point(\"agent\")\n",
    "workflow.set_finish_point(\"node_2\")\n",
    "\n",
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Agent Says: Las Vegas'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "app.invoke(\"What's the temperature in Las Vegas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Adding a weather API call\n",
    "\n",
    "What if we want the function 2 to take the city name and give us the weather for that city.\n",
    "\n",
    "Well we know that Open Weather Map is [integrated](https://python.langchain.com/docs/integrations/tools/openweathermap) into LangChain\n",
    "\n",
    "We need to install pyown, create an API key on the website of Open Weather Map (which takes a few hours to activate) and then run the cells below to get weather of a given city."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyowm\n",
      "  Obtaining dependency information for pyowm from https://files.pythonhosted.org/packages/6e/88/1279817aa7f5988a2ff42a6755fd371f3c1806aca377cb63b3d16684b174/pyowm-3.3.0-py3-none-any.whl.metadata\n",
      "  Downloading pyowm-3.3.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: requests<3,>=2.20.0 in ./.langgraph-notebook/lib/python3.11/site-packages (from pyowm) (2.31.0)\n",
      "Collecting geojson<3,>=2.3.0 (from pyowm)\n",
      "  Obtaining dependency information for geojson<3,>=2.3.0 from https://files.pythonhosted.org/packages/e4/8d/9e28e9af95739e6d2d2f8d4bef0b3432da40b7c3588fbad4298c1be09e48/geojson-2.5.0-py2.py3-none-any.whl.metadata\n",
      "  Downloading geojson-2.5.0-py2.py3-none-any.whl.metadata (15 kB)\n",
      "Collecting PySocks<2,>=1.7.1 (from pyowm)\n",
      "  Obtaining dependency information for PySocks<2,>=1.7.1 from https://files.pythonhosted.org/packages/8d/59/b4572118e098ac8e46e399a1dd0f2d85403ce8bbaad9ec79373ed6badaf9/PySocks-1.7.1-py3-none-any.whl.metadata\n",
      "  Downloading PySocks-1.7.1-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.langgraph-notebook/lib/python3.11/site-packages (from requests<3,>=2.20.0->pyowm) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.langgraph-notebook/lib/python3.11/site-packages (from requests<3,>=2.20.0->pyowm) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.langgraph-notebook/lib/python3.11/site-packages (from requests<3,>=2.20.0->pyowm) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.langgraph-notebook/lib/python3.11/site-packages (from requests<3,>=2.20.0->pyowm) (2024.2.2)\n",
      "Downloading pyowm-3.3.0-py3-none-any.whl (4.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m58.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading geojson-2.5.0-py2.py3-none-any.whl (14 kB)\n",
      "Downloading PySocks-1.7.1-py3-none-any.whl (16 kB)\n",
      "Installing collected packages: geojson, PySocks, pyowm\n",
      "Successfully installed PySocks-1.7.1 geojson-2.5.0 pyowm-3.3.0\n"
     ]
    }
   ],
   "source": [
    "!pip install pyowm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.utilities import OpenWeatherMapAPIWrapper\n",
    "load_dotenv()\n",
    "os.environ[\"OPENWEATHERMAP_API_KEY\"] = os.environ.get(\"OPENWEATHERMAP_API_KEY\")\n",
    "\n",
    "weather = OpenWeatherMapAPIWrapper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In Las Vegas, the current weather is as follows:\n",
      "Detailed status: clear sky\n",
      "Wind speed: 0 m/s, direction: 0°\n",
      "Humidity: 24%\n",
      "Temperature: \n",
      "  - Current: 22.18°C\n",
      "  - High: 23.23°C\n",
      "  - Low: 20.24°C\n",
      "  - Feels like: 21.08°C\n",
      "Rain: {}\n",
      "Heat index: None\n",
      "Cloud cover: 0%\n"
     ]
    }
   ],
   "source": [
    "weather_data = weather.run(\"Las Vegas\")\n",
    "print(weather_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's integrate this into function 2 and call the function two as a \"tool\" or \"weather_agent\" instead of \"node_2\" in our workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_1(input_1):\n",
    "    complete_query = \"Your task is to provide only the city name based on the user query. \\\n",
    "        Nothing more, just the city name mentioned. Following is the user query: \" + input_1\n",
    "    response = model.invoke(complete_query)\n",
    "    return response.content\n",
    "\n",
    "def function_2(input_2):\n",
    "    weather_data = weather.run(input_2)\n",
    "    return weather_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import Graph\n",
    "\n",
    "workflow = Graph()\n",
    "\n",
    "#calling node 1 as agent\n",
    "workflow.add_node(\"agent\", function_1)\n",
    "workflow.add_node(\"tool\", function_2)\n",
    "\n",
    "workflow.add_edge('agent', 'tool')\n",
    "\n",
    "workflow.set_entry_point(\"agent\")\n",
    "workflow.set_finish_point(\"tool\")\n",
    "\n",
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'In Las Vegas, the current weather is as follows:\\nDetailed status: clear sky\\nWind speed: 0 m/s, direction: 0°\\nHumidity: 24%\\nTemperature: \\n  - Current: 22.18°C\\n  - High: 23.23°C\\n  - Low: 20.24°C\\n  - Feels like: 21.08°C\\nRain: {}\\nHeat index: None\\nCloud cover: 0%'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "app.invoke(\"What's the temperature in Las Vegas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output from node 'agent':\n",
      "---\n",
      "Las Vegas\n",
      "\n",
      "---\n",
      "\n",
      "Output from node 'tool':\n",
      "---\n",
      "In Las Vegas, the current weather is as follows:\n",
      "Detailed status: clear sky\n",
      "Wind speed: 0 m/s, direction: 0°\n",
      "Humidity: 24%\n",
      "Temperature: \n",
      "  - Current: 22.18°C\n",
      "  - High: 23.23°C\n",
      "  - Low: 20.24°C\n",
      "  - Feels like: 21.08°C\n",
      "Rain: {}\n",
      "Heat index: None\n",
      "Cloud cover: 0%\n",
      "\n",
      "---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input = \"What's the temperature in Las Vegas\"\n",
    "for output in app.stream(input):\n",
    "    # stream() yields dictionaries with output keyed by node name\n",
    "    for key, value in output.items():\n",
    "        print(f\"Output from node '{key}':\")\n",
    "        print(\"---\")\n",
    "        print(value)\n",
    "    print(\"\\n---\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3 Adding another LLM Call to filter results\n",
    "\n",
    "What if we only want the temperature? But current setup gives us the full weather report. \n",
    "\n",
    "Well we can make another LLM call to filter data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_3(input_3):\n",
    "    complete_query = \"Your task is to provide info concisely based on the user query. You must also convert the temperature from celsius to Farenheit. Following is the user query: \" + \"user input\"\n",
    "    response = model.invoke(complete_query)\n",
    "    return response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But the issue is the user input is not available from node 2.\n",
    "\n",
    "Can we pass user input all along from first node to the last?\n",
    "\n",
    "Yes, we can use a dictionary and pass it between nodes (we could also use just a list, but dict makes it a bit easier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign AgentState as an empty dict\n",
    "AgentState = {}\n",
    "\n",
    "# messages key will be assigned as an empty array. We will append new messages as we pass along nodes. \n",
    "AgentState[\"messages\"] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': []}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AgentState"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our goal is to have this state filled as:\n",
    "{'messages': [HumanMessage, AIMessage, ...]]}\n",
    "\n",
    "Also now we need to modify our functions to pass info along the new AgentState"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_1(state):\n",
    "    messages = state['messages']\n",
    "    user_input = messages[-1]\n",
    "    complete_query = \"Your task is to provide only the city name based on the user query. \\\n",
    "                    Nothing more, just the city name mentioned. Following is the user query: \" + user_input\n",
    "    response = model.invoke(complete_query)\n",
    "    state['messages'].append(response.content) # appending AIMessage response to the AgentState\n",
    "    return state\n",
    "\n",
    "def function_2(state):\n",
    "    messages = state['messages']\n",
    "    agent_response = messages[-1]\n",
    "    weather = OpenWeatherMapAPIWrapper()\n",
    "    weather_data = weather.run(agent_response)\n",
    "    state['messages'].append(weather_data)\n",
    "    return state\n",
    "\n",
    "def function_3(state):\n",
    "    messages = state['messages']\n",
    "    user_input = messages[0]\n",
    "    available_info = messages[-1]\n",
    "    agent2_query = \"Your task is to provide info concisely based on the user query and the available information from the internet. \\\n",
    "                        You must also convert the temperature from celsius to Farenheit. Following is the user query: \" + user_input + \" Available information: \" + available_info\n",
    "    response = model.invoke(agent2_query)\n",
    "    return response.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import Graph\n",
    "\n",
    "workflow = Graph()\n",
    "\n",
    "\n",
    "workflow.add_node(\"agent\", function_1)\n",
    "workflow.add_node(\"tool\", function_2)\n",
    "workflow.add_node(\"responder\", function_3)\n",
    "\n",
    "workflow.add_edge('agent', 'tool')\n",
    "workflow.add_edge('tool', 'responder')\n",
    "\n",
    "workflow.set_entry_point(\"agent\")\n",
    "workflow.set_finish_point(\"responder\")\n",
    "\n",
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The current temperature in Las Vegas is 72.5°F.'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = {\"messages\": [\"what is the temperature in las vegas\"]}\n",
    "app.invoke(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output from node 'agent':\n",
      "---\n",
      "{'messages': ['what is the temperature in las vegas', 'Las Vegas']}\n",
      "\n",
      "---\n",
      "\n",
      "Output from node 'tool':\n",
      "---\n",
      "{'messages': ['what is the temperature in las vegas', 'Las Vegas', 'In Las Vegas, the current weather is as follows:\\nDetailed status: clear sky\\nWind speed: 0 m/s, direction: 0°\\nHumidity: 24%\\nTemperature: \\n  - Current: 22.5°C\\n  - High: 23.79°C\\n  - Low: 20.24°C\\n  - Feels like: 21.43°C\\nRain: {}\\nHeat index: None\\nCloud cover: 0%']}\n",
      "\n",
      "---\n",
      "\n",
      "Output from node 'responder':\n",
      "---\n",
      "The current temperature in Las Vegas is 22.5°C with clear skies, a low of 20.24°C, and a high of 23.79°C. The humidity is at 24% with no rain expected.\n",
      "\n",
      "---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input = {\"messages\": [\"what is the temperature in las vegas\"]}\n",
    "for output in app.stream(input):\n",
    "    # stream() yields dictionaries with output keyed by node name\n",
    "    for key, value in output.items():\n",
    "        print(f\"Output from node '{key}':\")\n",
    "        print(\"---\")\n",
    "        print(value)\n",
    "    print(\"\\n---\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we notice that there is a lot of appending to the array going on, we can make it a bit easier with the following:\n",
    "\n",
    "```bash\n",
    "from typing import TypedDict, Annotated, Sequence\n",
    "import operator\n",
    "from langchain_core.messages import BaseMessage\n",
    "\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[Sequence[BaseMessage], operator.add]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It basically makes the state dictionary as saw previously, and also makes sure that any new message is appended to the messages array when we do the following: \n",
    "```bash\n",
    "{\"messages\": [new_array_element]}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##### We also realize that our app is not capable of answering simple questions like \"how are you?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The current temperature in Dallas, TX is 12.18°C. Converting this to Fahrenheit, the temperature is approximately 53.92°F.'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = {\"messages\": [\"how are you?\"]}\n",
    "app.invoke(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is because we always want to parse a city and then find the weather. \n",
    "\n",
    "We can make our agent smarter by saying only use the tool when needed, if not just respond back to the user. \n",
    "\n",
    "The way we can do this LangGraph is:\n",
    "1. binding a tool to the agent\n",
    "2. adding a conditional edge to the agent with the option to either call the tool or not\n",
    "3. defining the criteria for the conditional edge as when to call the tool. We will define a function for this.\n",
    "\n",
    "\n",
    "Let's start with the AgentState definition as mentioned a few cells above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict, Annotated, Sequence\n",
    "import operator\n",
    "from langchain_core.messages import BaseMessage\n",
    "\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[Sequence[BaseMessage], operator.add]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Binding tool with agent (LLM Model) is made easy in langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.utils.function_calling import convert_to_openai_function\n",
    "from langchain_community.tools.openweathermap import OpenWeatherMapQueryRun\n",
    "from langchain_core.utils.function_calling import convert_to_openai_function\n",
    "\n",
    "tools = [OpenWeatherMapQueryRun()]\n",
    "\n",
    "model = ChatOpenAI(temperature=0, streaming=True)\n",
    "functions = [convert_to_openai_function(t) for t in tools]\n",
    "model = model.bind_functions(functions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our modified function_1 now becomes as below. The reason is, we are passing the human message as state and appending response to the state. Also, our agent now has a tool bound to it, that it can use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_1(state):\n",
    "    messages = state['messages']\n",
    "    response = model.invoke(messages)\n",
    "    return {\"messages\": [response]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For function 2, we want it to setup a tool and call it. It's made easy to invoke a tool in LangChain by using ToolInvocation and executing it with ToolExecuter. Then we respond back as a FunctionMessage so that our agent (node 1) knows that the tool was used and a response from tool is available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.prebuilt import ToolInvocation\n",
    "import json\n",
    "from langchain_core.messages import FunctionMessage\n",
    "from langgraph.prebuilt import ToolExecutor\n",
    "\n",
    "tool_executor = ToolExecutor(tools)\n",
    "\n",
    "def function_2(state):\n",
    "    messages = state['messages']\n",
    "    last_message = messages[-1] # this has the query we need to send to the tool provided by the agent\n",
    "\n",
    "    parsed_tool_input = json.loads(last_message.additional_kwargs[\"function_call\"][\"arguments\"])\n",
    "\n",
    "    # We construct an ToolInvocation from the function_call and pass in the tool name and the expected str input for OpenWeatherMap tool\n",
    "    action = ToolInvocation(\n",
    "        tool=last_message.additional_kwargs[\"function_call\"][\"name\"],\n",
    "        tool_input=parsed_tool_input['__arg1'],\n",
    "    )\n",
    "    \n",
    "    # We call the tool_executor and get back a response\n",
    "    response = tool_executor.invoke(action)\n",
    "\n",
    "    # We use the response to create a FunctionMessage\n",
    "    function_message = FunctionMessage(content=str(response), name=action.tool)\n",
    "\n",
    "    # We return a list, because this will get added to the existing list\n",
    "    return {\"messages\": [function_message]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we define a function for the conditional edge, to help us figure out which direction to go (tool or user response)\n",
    "\n",
    "We can benefit from the agent (LLM) response in LangChain, which has additional_kwargs to make a function_call with the name of the tool.\n",
    "\n",
    "So our logic is, if function_call available in the additional_kwargs, then call tool if not then end the discussion and respond back to the user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def where_to_go(state):\n",
    "    messages = state['messages']\n",
    "    last_message = messages[-1]\n",
    "    \n",
    "    if \"function_call\" in last_message.additional_kwargs:\n",
    "        return \"continue\"\n",
    "    else:\n",
    "        return \"end\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now with all of the changes above, our LangGraph app is modified as below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langgraph.graph import Graph, END\n",
    "\n",
    "# workflow = Graph()\n",
    "\n",
    "# Or you could import StateGraph and pass AgentState to it\n",
    "from langgraph.graph import StateGraph, END\n",
    "workflow = StateGraph(AgentState)\n",
    "\n",
    "workflow.add_node(\"agent\", function_1)\n",
    "workflow.add_node(\"tool\", function_2)\n",
    "\n",
    "# The conditional edge requires the following info below.\n",
    "# First, we define the start node. We use `agent`.\n",
    "# This means these are the edges taken after the `agent` node is called.\n",
    "# Next, we pass in the function that will determine which node is called next, in our case where_to_go().\n",
    "\n",
    "workflow.add_conditional_edges(\"agent\", where_to_go,{   # Based on the return from where_to_go\n",
    "                                                        # If return is \"continue\" then we call the tool node.\n",
    "                                                        \"continue\": \"tool\",\n",
    "                                                        # Otherwise we finish. END is a special node marking that the graph should finish.\n",
    "                                                        \"end\": END\n",
    "                                                    }\n",
    ")\n",
    "\n",
    "# We now add a normal edge from `tools` to `agent`.\n",
    "# This means that if `tool` is called, then it has to call the 'agent' next. \n",
    "workflow.add_edge('tool', 'agent')\n",
    "\n",
    "# Basically, agent node has the option to call a tool node based on a condition, \n",
    "# whereas tool node must call the agent in all cases based on this setup.\n",
    "\n",
    "workflow.set_entry_point(\"agent\")\n",
    "\n",
    "\n",
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also pass the first message using HumanMessage component available in langchain, makes it easy to differentiate from AIMessage, and FunctionMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='what is the temperature in Dallas in degrees farhenheit?'),\n",
       "  AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\"__arg1\":\"Dallas\"}', 'name': 'open_weather_map'}}, response_metadata={'finish_reason': 'function_call'}, id='run-9ac9e4ca-34b3-4393-93c5-4f32c1d28191-0'),\n",
       "  FunctionMessage(content=\"In Dallas, the current weather is as follows:\\nDetailed status: moderate rain\\nWind speed: 4.63 m/s, direction: 50°\\nHumidity: 95%\\nTemperature: \\n  - Current: 12.37°C\\n  - High: 13.42°C\\n  - Low: 11.41°C\\n  - Feels like: 12.14°C\\nRain: {'1h': 3.18}\\nHeat index: None\\nCloud cover: 100%\", name='open_weather_map'),\n",
       "  AIMessage(content='The current temperature in Dallas is 12.37°C. Would you like me to convert it to Fahrenheit for you?', response_metadata={'finish_reason': 'stop'}, id='run-dbf65a7d-3abc-40e6-be45-bd1f63c7144e-0')]}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "inputs = {\"messages\": [HumanMessage(content=\"what is the temperature in Dallas in degrees farhenheit?\")]}\n",
    "app.invoke(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output from node 'agent':\n",
      "---\n",
      "{'messages': [AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\"__arg1\":\"Las Vegas\"}', 'name': 'open_weather_map'}}, response_metadata={'finish_reason': 'function_call'}, id='run-4eff64ee-2334-48d8-897c-6e2fb3518d5e-0')]}\n",
      "\n",
      "---\n",
      "\n",
      "Output from node 'tool':\n",
      "---\n",
      "{'messages': [FunctionMessage(content='In Las Vegas, the current weather is as follows:\\nDetailed status: clear sky\\nWind speed: 2.24 m/s, direction: 290°\\nHumidity: 28%\\nTemperature: \\n  - Current: 20.82°C\\n  - High: 24.27°C\\n  - Low: 18.38°C\\n  - Feels like: 19.69°C\\nRain: {}\\nHeat index: None\\nCloud cover: 0%', name='open_weather_map')]}\n",
      "\n",
      "---\n",
      "\n",
      "Output from node 'agent':\n",
      "---\n",
      "{'messages': [AIMessage(content='The current temperature in Las Vegas is 20.82°C.', response_metadata={'finish_reason': 'stop'}, id='run-5ad7536f-d416-4686-afd4-9968d0c19ecf-0')]}\n",
      "\n",
      "---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inputs = {\"messages\": [HumanMessage(content=\"what is the temperature in las vegas\")]}\n",
    "for output in app.stream(inputs):\n",
    "    # stream() yields dictionaries with output keyed by node name\n",
    "    for key, value in output.items():\n",
    "        print(f\"Output from node '{key}':\")\n",
    "        print(\"---\")\n",
    "        print(value)\n",
    "    print(\"\\n---\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hopefully, that gives you a good understanding of how we built a LangGraph app and why we used different LC components."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
